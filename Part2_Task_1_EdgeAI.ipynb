{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b839e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/alistairking/recyclable-and-household-waste-classification?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 920M/920M [12:17<00:00, 1.31MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Caroline Mwanzia\\.cache\\kagglehub\\datasets\\alistairking\\recyclable-and-household-waste-classification\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"alistairking/recyclable-and-household-waste-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964874f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93ff3a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 files belonging to 30 classes.\n",
      "Using 12000 files for training.\n",
      "Found 15000 files belonging to 30 classes.\n",
      "Using 3000 files for validation.\n",
      "Classes: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste', 'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines', 'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles', 'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws', 'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups', 'styrofoam_food_containers', 'tea_bags']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import kagglehub # Added import\n",
    "\n",
    "# Ensure the dataset is downloaded and get its base path\n",
    "path = kagglehub.dataset_download(\"alistairking/recyclable-and-household-waste-classification\")\n",
    "dataset_base_path = path # Use the actual returned path\n",
    "\n",
    "# Correcting the path to the actual class subdirectories\n",
    "dataset_path = os.path.join(dataset_base_path, 'images', 'images')\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "print(\"Classes:\", train_ds.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d099f71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste', 'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines', 'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles', 'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws', 'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups', 'styrofoam_food_containers', 'tea_bags']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41203918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Caroline Mwanzia\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\layers\\preprocessing\\data_layer.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 147ms/step - accuracy: 0.1192 - loss: 3.1942 - val_accuracy: 0.2260 - val_loss: 2.8012\n",
      "Epoch 2/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 51ms/step - accuracy: 0.3349 - loss: 2.3690 - val_accuracy: 0.3883 - val_loss: 2.2744\n",
      "Epoch 3/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 195ms/step - accuracy: 0.5133 - loss: 1.7353 - val_accuracy: 0.4617 - val_loss: 2.0564\n",
      "Epoch 4/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 297ms/step - accuracy: 0.6492 - loss: 1.2438 - val_accuracy: 0.5173 - val_loss: 1.9566\n",
      "Epoch 5/5\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 54ms/step - accuracy: 0.7503 - loss: 0.8712 - val_accuracy: 0.5650 - val_loss: 1.9875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26cd63b5940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255, input_shape=(128,128,3)),\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(train_ds.class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e25f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\CAROLI~1\\AppData\\Local\\Temp\\tmpg_eajee9\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\CAROLI~1\\AppData\\Local\\Temp\\tmpg_eajee9\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\CAROLI~1\\AppData\\Local\\Temp\\tmpg_eajee9'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 30), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2666458787728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2666458789840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2666458788496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2666458789456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2666458789648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2666458790224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2666458790032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2666458790608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"recyclables_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be52bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: plastic_cup_lids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Caroline Mwanzia\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"recyclables_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Construct path to an example image file from the 'plastic_soda_bottles' class\n",
    "class_name_to_test = 'plastic_soda_bottles'\n",
    "original_class_dir_path = os.path.join(dataset_path, class_name_to_test)\n",
    "\n",
    "# Assuming images are in a 'default' subdirectory within the class directory\n",
    "image_source_dir = os.path.join(original_class_dir_path, 'default')\n",
    "\n",
    "# Find the first image file in this directory\n",
    "# Assuming there are jpg, jpeg, or png files in the directory\n",
    "image_files = [f for f in os.listdir(image_source_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "if not image_files:\n",
    "    raise FileNotFoundError(f\"No image files found in {image_source_dir}\")\n",
    "\n",
    "example_image_path = os.path.join(image_source_dir, image_files[0])\n",
    "\n",
    "img = tf.keras.utils.load_img(example_image_path, target_size=(128,128))\n",
    "img_array = tf.keras.utils.img_to_array(img)/255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], img_array.astype(np.float32))\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "predicted_class = train_ds.class_names[np.argmax(output_data)]\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53977994",
   "metadata": {},
   "source": [
    "üöÄ Benefits of Edge AI in Real-Time Recycling\n",
    "‚ö° Instant decisions: Classify items in milliseconds without cloud delays.\n",
    "\n",
    "üîí Privacy: Camera images stay local, not uploaded to external servers.\n",
    "\n",
    "üåê Offline capability: Works even without internet (critical for smart bins in rural areas).\n",
    "\n",
    "üîã Efficiency: Lightweight models run on Raspberry Pi with low power consumption.\n",
    "\n",
    "üëâ Example: A smart recycling bin with a Pi camera can instantly tell whether an item is plastic, paper, or glass, guiding users to dispose correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
